# ETL Pipeline using PySpark

This project demonstrates a basic ETL (Extract, Transform, Load) pipeline using PySpark. It reads a dataset, performs data cleaning and transformation, and finally writes the clean data to a destination.

## Features

- Extracts data from a CSV file using PySpark
- Handles null values and casts data types appropriately
- Performs transformation on fields (e.g., trimming whitespace, renaming columns)
- Writes the transformed data to a new CSV

## Technologies Used

- Python
- Apache Spark (PySpark)
- Jupyter Notebook

## Getting Started

### Prerequisites

- Python 3.x
- Java 8 or later
- Apache Spark
- Jupyter Notebook

### Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/etl-pyspark.git
   cd etl-pyspark
